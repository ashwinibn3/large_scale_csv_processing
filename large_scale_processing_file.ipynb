{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Large-Scale CSV Data Processing and Validation Report\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This report documents the process of generating, processing, validating, and exporting a large CSV dataset using Python in Google Colab. The goal is to demonstrate computational efficiency using multiple reading tools, clean and validate column names, create a schema in YAML, and export the cleaned data in a compressed pipe-separated format."
      ],
      "metadata": {
        "id": "8uTMnKWqyfs1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Created a 2GB CSV File in Chunks\n",
        "\n",
        "I generated a synthetic dataset with 50 million rows and 5 columns (`id`, `name`, `age`, `salary`, `city`) using NumPy and pandas. To avoid memory errors in Colab, I wrote the data in chunks of 1 million rows. The first chunk included the header, and the remaining chunks were appended without headers.\n",
        "\n",
        "This process produced a large CSV file (`large_dataset_file.csv`) of approximately 2GB in size.\n"
      ],
      "metadata": {
        "id": "7SPV6i3Qz_Rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# üîÅ Total rows and chunk size\n",
        "total_rows = 50_000_000\n",
        "chunk_size = 1_000_000\n",
        "\n",
        "#  First chunk: write with headers\n",
        "df = pd.DataFrame({\n",
        "    'id': np.arange(0, chunk_size),\n",
        "    'name': np.random.choice(['Alice', 'Bob', 'Charlie'], size=chunk_size),\n",
        "    'age': np.random.randint(18, 70, size=chunk_size),\n",
        "    'salary': np.random.uniform(30000, 120000, size=chunk_size),\n",
        "    'city': np.random.choice(['London', 'New York', 'Delhi'], size=chunk_size)\n",
        "})\n",
        "\n",
        "df.to_csv('large_dataset_file.csv', index=False, mode='w')  # First write (with header)\n",
        "\n",
        "# ‚è≥ Now write the rest in append mode\n",
        "for i in range(1, total_rows // chunk_size):\n",
        "    start = i * chunk_size\n",
        "    df = pd.DataFrame({\n",
        "        'id': np.arange(start, start + chunk_size),\n",
        "        'name': np.random.choice(['Alice', 'Bob', 'Charlie'], size=chunk_size),\n",
        "        'age': np.random.randint(18, 70, size=chunk_size),\n",
        "        'salary': np.random.uniform(30000, 120000, size=chunk_size),\n",
        "        'city': np.random.choice(['London', 'New York', 'Delhi'], size=chunk_size)\n",
        "    })\n",
        "\n",
        "    # Append to CSV without writing header again\n",
        "    df.to_csv('large_dataset_file.csv', index=False, mode='a', header=False)\n",
        "\n",
        "    print(f\" Written chunk {i+1} of {total_rows // chunk_size}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG5HDQoPl8o4",
        "outputId": "97be0996-99c8-4a1c-f4cc-63538b7825f6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Written chunk 2 of 50\n",
            " Written chunk 3 of 50\n",
            " Written chunk 4 of 50\n",
            " Written chunk 5 of 50\n",
            " Written chunk 6 of 50\n",
            " Written chunk 7 of 50\n",
            " Written chunk 8 of 50\n",
            " Written chunk 9 of 50\n",
            " Written chunk 10 of 50\n",
            " Written chunk 11 of 50\n",
            " Written chunk 12 of 50\n",
            " Written chunk 13 of 50\n",
            " Written chunk 14 of 50\n",
            " Written chunk 15 of 50\n",
            " Written chunk 16 of 50\n",
            " Written chunk 17 of 50\n",
            " Written chunk 18 of 50\n",
            " Written chunk 19 of 50\n",
            " Written chunk 20 of 50\n",
            " Written chunk 21 of 50\n",
            " Written chunk 22 of 50\n",
            " Written chunk 23 of 50\n",
            " Written chunk 24 of 50\n",
            " Written chunk 25 of 50\n",
            " Written chunk 26 of 50\n",
            " Written chunk 27 of 50\n",
            " Written chunk 28 of 50\n",
            " Written chunk 29 of 50\n",
            " Written chunk 30 of 50\n",
            " Written chunk 31 of 50\n",
            " Written chunk 32 of 50\n",
            " Written chunk 33 of 50\n",
            " Written chunk 34 of 50\n",
            " Written chunk 35 of 50\n",
            " Written chunk 36 of 50\n",
            " Written chunk 37 of 50\n",
            " Written chunk 38 of 50\n",
            " Written chunk 39 of 50\n",
            " Written chunk 40 of 50\n",
            " Written chunk 41 of 50\n",
            " Written chunk 42 of 50\n",
            " Written chunk 43 of 50\n",
            " Written chunk 44 of 50\n",
            " Written chunk 45 of 50\n",
            " Written chunk 46 of 50\n",
            " Written chunk 47 of 50\n",
            " Written chunk 48 of 50\n",
            " Written chunk 49 of 50\n",
            " Written chunk 50 of 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Reading the Large CSV File Using Different Methods\n",
        "\n",
        "In this step, I tested how fast different tools can read the 2GB CSV file. I used four methods:\n",
        "\n",
        "- **Pandas**: the normal way we usually read CSV files.\n",
        "- **Dask**: reads large files in chunks using parallel processing.\n",
        "- **Modin with Ray**: speeds up pandas using multiple CPU cores.\n",
        "- **Ray (Direct)**: runs pandas as a remote task using Ray directly.\n",
        "\n",
        "For each one, I recorded how long it took and checked the shape of the data. This helped me understand which tools are better for working with large datasets.\n"
      ],
      "metadata": {
        "id": "bNKE_I1H0iAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "file_size = os.path.getsize('large_dataset_file.csv') / (1024 * 1024 * 1024)\n",
        "print(f\" Final file size: {file_size:.2f} GB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8CJ0yvim009",
        "outputId": "4d3c286a-d51a-4db0-8a52-1b706df84460"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Final file size: 2.02 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Dask\n",
        "!pip install dask -q\n",
        "\n",
        "# Install Modin with Ray backend\n",
        "!pip install modin[ray] -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IE2LNHa-oNe8",
        "outputId": "2849b5fd-4fb9-41e0-cae8-032133daa644"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m70.1/70.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "df_pandas = pd.read_csv('large_dataset_file.csv')\n",
        "\n",
        "end_time = time.time()\n",
        "print(\" Pandas Read Done\")\n",
        "print(\" Time Taken:\", round(end_time - start_time, 2), \"seconds\")\n",
        "print(\" Shape:\", df_pandas.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-CNyyBAomuU",
        "outputId": "a4b78af4-5da0-473e-a96b-e2ecdd6517dc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Pandas Read Done\n",
            " Time Taken: 37.43 seconds\n",
            " Shape: (50000000, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.dataframe as dd\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Read CSV with Dask (lazy)\n",
        "df_dask = dd.read_csv('large_dataset_file.csv')\n",
        "\n",
        "# Force computation\n",
        "row_count = len(df_dask)             # Triggers reading\n",
        "col_count = len(df_dask.columns)     # Already known\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"Dask Read Done\")\n",
        "print(\"Time Taken:\", round(end_time - start_time, 2), \"seconds\")\n",
        "print(\" Shape:\", (row_count, col_count))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_oM0vhopVo_",
        "outputId": "25b2a43d-ab37-4e69-8aa1-d58a84ed0ab2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dask Read Done\n",
            "Time Taken: 34.09 seconds\n",
            " Shape: (50000000, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import modin.pandas as mpd\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "df_modin = mpd.read_csv('large_dataset_file.csv')\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"Modin Read Done\")\n",
        "print(\"Time Taken:\", round(end_time - start_time, 2), \"seconds\")\n",
        "print(\" Shape:\", df_modin.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcYCnHhRpsFH",
        "outputId": "c9397a1b-9ca9-4967-eb4d-33ae7a1b8a25"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UserWarning: The size of /dev/shm is too small (6133121024 bytes). The required size at least half of RAM (6804185088 bytes). Please, delete files in /dev/shm or increase size of /dev/shm with --shm-size in Docker. Also, you can can override the memory size for each Ray worker (in bytes) to the MODIN_MEMORY environment variable.\n",
            "2025-07-26 12:13:24,910\tINFO worker.py:1927 -- Started a local Ray instance.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-07-26 12:14:24,871 E 6291 6291] (raylet) node_manager.cc:3041: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 2e2e17d1ed017cdfab688b9eed651a95386d43fd01b38b20cf3a6371, IP: 172.28.0.12) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.28.0.12`\n",
            "\u001b[33m(raylet)\u001b[0m \n",
            "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modin Read Done\n",
            "Time Taken: 93.31 seconds\n",
            " Shape: (50000000, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "import time\n",
        "\n",
        "# Initialize Ray if not already running\n",
        "ray.init(ignore_reinit_error=True)\n",
        "\n",
        "@ray.remote\n",
        "def read_with_ray():\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(\"large_dataset_file.csv\")\n",
        "    return df.shape\n",
        "\n",
        "start_time = time.time()\n",
        "shape_ray = ray.get(read_with_ray.remote())\n",
        "end_time = time.time()\n",
        "\n",
        "time_ray = round(end_time - start_time, 2)\n",
        "\n",
        "print(\"Ray (Direct) Read Done\")\n",
        "print(\"Time Taken:\", time_ray, \"seconds\")\n",
        "print(\" Shape:\", shape_ray)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGppG2fPrQyq",
        "outputId": "eae94511-6175-4b82-e5e7-adb80ff63413"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-26 12:20:12,614\tINFO worker.py:1765 -- Calling ray.init() again after it has already been called.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ray (Direct) Read Done\n",
            "Time Taken: 46.22 seconds\n",
            " Shape: (50000000, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 Summary: File Reading Time Comparison\n",
        "\n",
        "After testing all four methods (Pandas, Dask, Modin, and Ray), I created a table to compare how long each one took to read the 2GB CSV file. All methods returned the same number of rows and columns, but the time varied based on how they handle parallel processing.\n",
        "\n",
        "This table helped me understand the performance differences when working with large datasets.\n"
      ],
      "metadata": {
        "id": "K2tqAKls001c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "# Replace with your actual times\n",
        "time_pandas = 37.43\n",
        "time_dask = 34.09\n",
        "time_modin = 93.31\n",
        "# time_ray = captured from previous step\n",
        "\n",
        "summary_df = pd.DataFrame({\n",
        "    'Method': ['Pandas', 'Dask', 'Modin (Ray)', 'Ray (Direct)'],\n",
        "    'Time Taken (s)': [time_pandas, time_dask, time_modin, time_ray],\n",
        "    'Rows': [50000000]*4,\n",
        "    'Columns': [5]*4,\n",
        "    'Notes': [\n",
        "        'Standard single-threaded read',\n",
        "        'Lazy chunked read (parallel)',\n",
        "        'Ray backend, memory overhead',\n",
        "        'Used Ray remote worker directly'\n",
        "    ]\n",
        "})\n",
        "\n",
        "display(summary_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "KJ9_CUAErpTI",
        "outputId": "5f9c03f9-d42a-47ef-a418-1dfc956ad5be"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         Method  Time Taken (s)      Rows  Columns  \\\n",
              "0        Pandas           37.43  50000000        5   \n",
              "1          Dask           34.09  50000000        5   \n",
              "2   Modin (Ray)           93.31  50000000        5   \n",
              "3  Ray (Direct)           46.22  50000000        5   \n",
              "\n",
              "                             Notes  \n",
              "0    Standard single-threaded read  \n",
              "1     Lazy chunked read (parallel)  \n",
              "2     Ray backend, memory overhead  \n",
              "3  Used Ray remote worker directly  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-79132278-54e7-4cff-94c6-c60a8f57a801\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Method</th>\n",
              "      <th>Time Taken (s)</th>\n",
              "      <th>Rows</th>\n",
              "      <th>Columns</th>\n",
              "      <th>Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pandas</td>\n",
              "      <td>37.43</td>\n",
              "      <td>50000000</td>\n",
              "      <td>5</td>\n",
              "      <td>Standard single-threaded read</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dask</td>\n",
              "      <td>34.09</td>\n",
              "      <td>50000000</td>\n",
              "      <td>5</td>\n",
              "      <td>Lazy chunked read (parallel)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Modin (Ray)</td>\n",
              "      <td>93.31</td>\n",
              "      <td>50000000</td>\n",
              "      <td>5</td>\n",
              "      <td>Ray backend, memory overhead</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ray (Direct)</td>\n",
              "      <td>46.22</td>\n",
              "      <td>50000000</td>\n",
              "      <td>5</td>\n",
              "      <td>Used Ray remote worker directly</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79132278-54e7-4cff-94c6-c60a8f57a801')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-79132278-54e7-4cff-94c6-c60a8f57a801 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-79132278-54e7-4cff-94c6-c60a8f57a801');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-24abbcf5-a6a4-40fd-bc59-c910469cfbbc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-24abbcf5-a6a4-40fd-bc59-c910469cfbbc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-24abbcf5-a6a4-40fd-bc59-c910469cfbbc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_1987d8f3-1ef7-4863-98ea-006bc6db40d0\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('summary_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_1987d8f3-1ef7-4863-98ea-006bc6db40d0 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('summary_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "summary_df",
              "summary": "{\n  \"name\": \"summary_df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Method\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Dask\",\n          \"Ray (Direct)\",\n          \"Pandas\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Time Taken (s)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 27.511524100517104,\n        \"min\": 34.09,\n        \"max\": 93.31,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          34.09,\n          46.22,\n          37.43\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rows\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 50000000,\n        \"max\": 50000000,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          50000000\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Columns\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 5,\n        \"max\": 5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Notes\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Lazy chunked read (parallel)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Creating the YAML Schema\n",
        "\n",
        "Here, I created a simple YAML file (`schema.yaml`) that stores the column names and the delimiter (`|`) I plan to use when saving the cleaned file. This helps make sure the structure stays consistent later during validation and export.\n"
      ],
      "metadata": {
        "id": "JAhJUhnP1jgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read just the header + small sample\n",
        "df_sample = pd.read_csv('large_dataset_file.csv', nrows=5)\n",
        "print(df_sample.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOE_28BVtKnn",
        "outputId": "f7c2121f-c996-48ff-853b-028c93bae0e2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'name', 'age', 'salary', 'city'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyyaml -q\n"
      ],
      "metadata": {
        "id": "h5F-7csfuHhp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "# Define the schema\n",
        "schema = {\n",
        "    'delimiter': '|',    # You‚Äôll use this later when writing the file\n",
        "    'columns': ['id', 'name', 'age', 'salary', 'city']\n",
        "}\n"
      ],
      "metadata": {
        "id": "VxWWBZ6PuWW2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to schema.yaml\n",
        "with open('schema.yaml', 'w') as file:\n",
        "    yaml.dump(schema, file)\n"
      ],
      "metadata": {
        "id": "qoKddxMRuYj2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read back and display for confirmation\n",
        "with open('schema.yaml', 'r') as file:\n",
        "    print(file.read())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P96-59Wubrf",
        "outputId": "d32c382b-d79d-4034-c2de-fdae11a8f0e8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "columns:\n",
            "- id\n",
            "- name\n",
            "- age\n",
            "- salary\n",
            "- city\n",
            "delimiter: '|'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Validating Column Names with YAML Schema\n",
        "\n",
        "In this step, I loaded the YAML schema and compared it to the actual dataset to make sure the column names and count matched.\n",
        "\n",
        "I used only the first few rows for this check and confirmed that the dataset has exactly the same number of columns and the same column names as defined in the YAML file.\n"
      ],
      "metadata": {
        "id": "WosdwR4b13mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yaml\n"
      ],
      "metadata": {
        "id": "lYugck0ivRCO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load schema.yaml\n",
        "with open('schema.yaml', 'r') as file:\n",
        "    schema = yaml.safe_load(file)\n",
        "\n",
        "# Extract expected columns and delimiter\n",
        "expected_columns = schema['columns']\n",
        "expected_delimiter = schema['delimiter']\n",
        "\n",
        "print(\"‚úÖ Schema Loaded:\")\n",
        "print(\"Expected Columns:\", expected_columns)\n",
        "print(\"Expected Delimiter:\", repr(expected_delimiter))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkDIhMF8vTEM",
        "outputId": "123a68d4-7ad3-4209-e247-4f9be4daba92"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Schema Loaded:\n",
            "Expected Columns: ['id', 'name', 'age', 'salary', 'city']\n",
            "Expected Delimiter: '|'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ‚ö†Ô∏è Use the actual delimiter for this file (comma)\n",
        "df = pd.read_csv('large_dataset_file.csv', sep=',', nrows=5)\n",
        "\n",
        "# Get the column names from the file\n",
        "actual_columns = df.columns.tolist()\n",
        "print(\"üìÇ File Columns:\", actual_columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pqBAyI1v2P9",
        "outputId": "c202b5af-f329-4954-dd6a-76d064564cf8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ File Columns: ['id', 'name', 'age', 'salary', 'city']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Column count check\n",
        "if len(actual_columns) != len(expected_columns):\n",
        "    print(\"Column count mismatch!\")\n",
        "    print(f\"YAML: {len(expected_columns)} columns\")\n",
        "    print(f\"File: {len(actual_columns)} columns\")\n",
        "else:\n",
        "    print(\"Column count matches\")\n",
        "\n",
        "# Column names check\n",
        "if actual_columns != expected_columns:\n",
        "    print(\" Column names do NOT match the YAML schema\")\n",
        "    print(\"Mismatches:\")\n",
        "    for i, (actual, expected) in enumerate(zip(actual_columns, expected_columns)):\n",
        "        if actual != expected:\n",
        "            print(f\"  Column {i+1}: Expected '{expected}', got '{actual}'\")\n",
        "else:\n",
        "    print(\"Column names match exactly\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUBhG-XFv4RP",
        "outputId": "e06e3765-f9ee-43a7-e744-ad285bd4694f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column count matches\n",
            "Column names match exactly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Save the Cleaned File as Pipe-Separated GZIP\n",
        "\n",
        "After validating the column names, I saved the full dataset using `|` as the separator and compressed it using gzip format.\n",
        "\n",
        "The final file is named `cleaned_dataset_file.txt.gz`, which helps reduce size and ensures it's consistent with the YAML schema.\n"
      ],
      "metadata": {
        "id": "bx-3mKUM2Cf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read your original CSV file\n",
        "df = pd.read_csv('large_dataset_file.csv')  # This file uses ',' as delimiter\n"
      ],
      "metadata": {
        "id": "YgyWy4CvwZSn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame as a pipe-separated gzip file\n",
        "df.to_csv('cleaned_dataset_file.txt.gz', sep='|', index=False, compression='gzip')\n"
      ],
      "metadata": {
        "id": "fjl2WoeJwrp3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Summary of the Final File\n",
        "\n",
        "Here, I checked the final cleaned file to confirm everything looks correct. I printed the total number of rows and columns, and calculated the file size in both MB and GB.\n",
        "\n",
        "The output file `cleaned_dataset_file.txt.gz` contains 50 million rows and 5 columns, and its size is around 704 MB after gzip compression.\n"
      ],
      "metadata": {
        "id": "F_JS9PoL3Moa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Get shape of the dataframe\n",
        "rows, columns = df.shape\n",
        "print(f\"Total Rows: {rows}\")\n",
        "print(f\"Total Columns: {columns}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpS65dsdzPOA",
        "outputId": "eeacb2f5-7f72-4c35-d799-5b078ac65d48"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows: 50000000\n",
            "Total Columns: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the file size in bytes\n",
        "file_path = 'cleaned_dataset_file.txt.gz'\n",
        "file_size_bytes = os.path.getsize(file_path)\n",
        "\n",
        "# Convert to MB and GB\n",
        "file_size_mb = round(file_size_bytes / (1024 * 1024), 2)\n",
        "file_size_gb = round(file_size_bytes / (1024 * 1024 * 1024), 2)\n",
        "\n",
        "print(f\"File Size: {file_size_mb} MB ({file_size_gb} GB)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvHLUr0KzXWz",
        "outputId": "bb808e4c-9eaa-4840-ed0d-d2d2c533a62d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Size: 704.36 MB (0.69 GB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Conclusion\n",
        "\n",
        "In this project, I worked with a large synthetic dataset (50 million rows) to simulate a real-world data processing workflow. I compared different tools for reading large files, validated the data structure using a YAML schema, cleaned the data, and saved it in a compressed pipe-separated format.\n",
        "\n",
        "The final file was successfully saved as `cleaned_dataset_file.txt.gz` with the correct schema and size (~704 MB). This end-to-end process helped demonstrate how to handle big data efficiently using Python in a cloud notebook environment like Google Colab.\n"
      ],
      "metadata": {
        "id": "Lz798OtiyeNj"
      }
    }
  ]
}